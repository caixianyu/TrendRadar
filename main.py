# coding=utf-8

import json
import os
import random
import re
import time
import webbrowser
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.header import Header
from email.utils import formataddr, formatdate, make_msgid
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union

import pytz
import requests
import yaml


VERSION = "3.0.5"


# === SMTPé‚®ä»¶é…ç½® ===
SMTP_CONFIGS = {
    # Gmail(ä½¿ç”¨ STARTTLS)
    "gmail.com": {"server": "smtp.gmail.com", "port": 587, "encryption": "TLS"},
    # QQé‚®ç®±(ä½¿ç”¨ SSL,æ›´ç¨³å®š)
    "qq.com": {"server": "smtp.qq.com", "port": 465, "encryption": "SSL"},
    # Outlook(ä½¿ç”¨ STARTTLS)
    "outlook.com": {
        "server": "smtp-mail.outlook.com",
        "port": 587,
        "encryption": "TLS",
    },
    "hotmail.com": {
        "server": "smtp-mail.outlook.com",
        "port": 587,
        "encryption": "TLS",
    },
    "live.com": {"server": "smtp-mail.outlook.com", "port": 587, "encryption": "TLS"},
    # ç½‘æ˜“é‚®ç®±(ä½¿ç”¨ SSL,æ›´ç¨³å®š)
    "163.com": {"server": "smtp.163.com", "port": 465, "encryption": "SSL"},
    "126.com": {"server": "smtp.126.com", "port": 465, "encryption": "SSL"},
    # æ–°æµªé‚®ç®±(ä½¿ç”¨ SSL)
    "sina.com": {"server": "smtp.sina.com", "port": 465, "encryption": "SSL"},
    # æœç‹é‚®ç®±(ä½¿ç”¨ SSL)
    "sohu.com": {"server": "smtp.sohu.com", "port": 465, "encryption": "SSL"},
}


# === é…ç½®ç®¡ç† ===
def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = os.environ.get("CONFIG_PATH", "config/config.yaml")

    if not Path(config_path).exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨")

    with open(config_path, "r", encoding="utf-8") as f:
        config_data = yaml.safe_load(f)

    print(f"é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_path}")

    # æ„å»ºé…ç½®
    config = {
        "VERSION_CHECK_URL": config_data["app"]["version_check_url"],
        "SHOW_VERSION_UPDATE": config_data["app"]["show_version_update"],
        "REQUEST_INTERVAL": config_data["crawler"]["request_interval"],
        "REPORT_MODE": os.environ.get("REPORT_MODE", "").strip()
        or config_data["report"]["mode"],
        "RANK_THRESHOLD": config_data["report"]["rank_threshold"],
        "USE_PROXY": config_data["crawler"]["use_proxy"],
        "DEFAULT_PROXY": config_data["crawler"]["default_proxy"],
        "ENABLE_CRAWLER": os.environ.get("ENABLE_CRAWLER", "").strip().lower()
        in ("true", "1")
        if os.environ.get("ENABLE_CRAWLER", "").strip()
        else config_data["crawler"]["enable_crawler"],
        "ENABLE_NOTIFICATION": os.environ.get("ENABLE_NOTIFICATION", "").strip().lower()
        in ("true", "1")
        if os.environ.get("ENABLE_NOTIFICATION", "").strip()
        else config_data["notification"]["enable_notification"],
        "MESSAGE_BATCH_SIZE": config_data["notification"]["message_batch_size"],
        "DINGTALK_BATCH_SIZE": config_data["notification"].get(
            "dingtalk_batch_size", 20000
        ),
        "FEISHU_BATCH_SIZE": config_data["notification"].get("feishu_batch_size", 29000),
        "BATCH_SEND_INTERVAL": config_data["notification"]["batch_send_interval"],
        "FEISHU_MESSAGE_SEPARATOR": config_data["notification"][
            "feishu_message_separator"
        ],
        "PUSH_WINDOW": {
            "ENABLED": os.environ.get("PUSH_WINDOW_ENABLED", "").strip().lower()
            in ("true", "1")
            if os.environ.get("PUSH_WINDOW_ENABLED", "").strip()
            else config_data["notification"]
            .get("push_window", {})
            .get("enabled", False),
            "TIME_RANGE": {
                "START": os.environ.get("PUSH_WINDOW_START", "").strip()
                or config_data["notification"]
                .get("push_window", {})
                .get("time_range", {})
                .get("start", "08:00"),
                "END": os.environ.get("PUSH_WINDOW_END", "").strip()
                or config_data["notification"]
                .get("push_window", {})
                .get("time_range", {})
                .get("end", "22:00"),
            },
            "ONCE_PER_DAY": os.environ.get("PUSH_WINDOW_ONCE_PER_DAY", "").strip().lower()
            in ("true", "1")
            if os.environ.get("PUSH_WINDOW_ONCE_PER_DAY", "").strip()
            else config_data["notification"]
            .get("push_window", {})
            .get("once_per_day", True),
            "RECORD_RETENTION_DAYS": int(
                os.environ.get("PUSH_WINDOW_RETENTION_DAYS", "").strip() or "0"
            )
            or config_data["notification"]
            .get("push_window", {})
            .get("push_record_retention_days", 7),
        },
        "WEIGHT_CONFIG": {
            "RANK_WEIGHT": config_data["weight"]["rank_weight"],
            "FREQUENCY_WEIGHT": config_data["weight"]["frequency_weight"],
            "HOTNESS_WEIGHT": config_data["weight"]["hotness_weight"],
        },
        "PLATFORMS": config_data["platforms"],
    }

    # é€šçŸ¥æ¸ é“é…ç½®(ç¯å¢ƒå˜é‡ä¼˜å…ˆ)
    notification = config_data.get("notification", {})
    webhooks = notification.get("webhooks", {})

    config["FEISHU_WEBHOOK_URL"] = os.environ.get(
        "FEISHU_WEBHOOK_URL", ""
    ).strip() or webhooks.get("feishu_url", "")
    config["DINGTALK_WEBHOOK_URL"] = os.environ.get(
        "DINGTALK_WEBHOOK_URL", ""
    ).strip() or webhooks.get("dingtalk_url", "")
    config["WEWORK_WEBHOOK_URL"] = os.environ.get(
        "WEWORK_WEBHOOK_URL", ""
    ).strip() or webhooks.get("wework_url", "")
    config["TELEGRAM_BOT_TOKEN"] = os.environ.get(
        "TELEGRAM_BOT_TOKEN", ""
    ).strip() or webhooks.get("telegram_bot_token", "")
    config["TELEGRAM_CHAT_ID"] = os.environ.get(
        "TELEGRAM_CHAT_ID", ""
    ).strip() or webhooks.get("telegram_chat_id", "")

    # é‚®ä»¶é…ç½®
    config["EMAIL_FROM"] = os.environ.get("EMAIL_FROM", "").strip() or webhooks.get(
        "email_from", ""
    )
    config["EMAIL_PASSWORD"] = os.environ.get(
        "EMAIL_PASSWORD", ""
    ).strip() or webhooks.get("email_password", "")
    config["EMAIL_TO"] = os.environ.get("EMAIL_TO", "").strip() or webhooks.get(
        "email_to", ""
    )
    config["EMAIL_SMTP_SERVER"] = os.environ.get(
        "EMAIL_SMTP_SERVER", ""
    ).strip() or webhooks.get("email_smtp_server", "")
    config["EMAIL_SMTP_PORT"] = os.environ.get(
        "EMAIL_SMTP_PORT", ""
    ).strip() or webhooks.get("email_smtp_port", "")

    # ntfyé…ç½®
    config["NTFY_SERVER_URL"] = os.environ.get(
        "NTFY_SERVER_URL", "https://ntfy.sh"
    ).strip() or webhooks.get("ntfy_server_url", "https://ntfy.sh")
    config["NTFY_TOPIC"] = os.environ.get("NTFY_TOPIC", "").strip() or webhooks.get(
        "ntfy_topic", ""
    )
    config["NTFY_TOKEN"] = os.environ.get("NTFY_TOKEN", "").strip() or webhooks.get(
        "ntfy_token", ""
    )

    # è¾“å‡ºé…ç½®æ¥æºä¿¡æ¯
    notification_sources = []
    if config["FEISHU_WEBHOOK_URL"]:
        source = "ç¯å¢ƒå˜é‡" if os.environ.get("FEISHU_WEBHOOK_URL") else "é…ç½®æ–‡ä»¶"
        notification_sources.append(f"é£ä¹¦({source})")
    if config["DINGTALK_WEBHOOK_URL"]:
        source = "ç¯å¢ƒå˜é‡" if os.environ.get("DINGTALK_WEBHOOK_URL") else "é…ç½®æ–‡ä»¶"
        notification_sources.append(f"é’‰é’‰({source})")
    if config["WEWORK_WEBHOOK_URL"]:
        source = "ç¯å¢ƒå˜é‡" if os.environ.get("WEWORK_WEBHOOK_URL") else "é…ç½®æ–‡ä»¶"
        notification_sources.append(f"ä¼ä¸šå¾®ä¿¡({source})")
    if config["TELEGRAM_BOT_TOKEN"] and config["TELEGRAM_CHAT_ID"]:
        token_source = (
            "ç¯å¢ƒå˜é‡" if os.environ.get("TELEGRAM_BOT_TOKEN") else "é…ç½®æ–‡ä»¶"
        )
        chat_source = "ç¯å¢ƒå˜é‡" if os.environ.get("TELEGRAM_CHAT_ID") else "é…ç½®æ–‡ä»¶"
        notification_sources.append(f"Telegram({token_source}/{chat_source})")
    if config["EMAIL_FROM"] and config["EMAIL_PASSWORD"] and config["EMAIL_TO"]:
        from_source = "ç¯å¢ƒå˜é‡" if os.environ.get("EMAIL_FROM") else "é…ç½®æ–‡ä»¶"
        notification_sources.append(f"é‚®ä»¶({from_source})")

    if config["NTFY_SERVER_URL"] and config["NTFY_TOPIC"]:
        server_source = "ç¯å¢ƒå˜é‡" if os.environ.get("NTFY_SERVER_URL") else "é…ç½®æ–‡ä»¶"
        notification_sources.append(f"ntfy({server_source})")

    if notification_sources:
        print(f"é€šçŸ¥æ¸ é“é…ç½®æ¥æº: {', '.join(notification_sources)}")
    else:
        print("æœªé…ç½®ä»»ä½•é€šçŸ¥æ¸ é“")

    return config


print("æ­£åœ¨åŠ è½½é…ç½®...")
CONFIG = load_config()
print(f"TrendRadar v{VERSION} é…ç½®åŠ è½½å®Œæˆ")
print(f"ç›‘æ§å¹³å°æ•°é‡: {len(CONFIG['PLATFORMS'])}")


# === å·¥å…·å‡½æ•° ===
def get_beijing_time():
    """è·å–åŒ—äº¬æ—¶é—´"""
    return datetime.now(pytz.timezone("Asia/Shanghai"))


def format_date_folder():
    """æ ¼å¼åŒ–æ—¥æœŸæ–‡ä»¶å¤¹"""
    return get_beijing_time().strftime("%Yå¹´%mæœˆ%dæ—¥")


def format_time_filename():
    """æ ¼å¼åŒ–æ—¶é—´æ–‡ä»¶å"""
    return get_beijing_time().strftime("%Hæ—¶%Måˆ†")


def clean_title(title: str) -> str:
    """æ¸…ç†æ ‡é¢˜ä¸­çš„ç‰¹æ®Šå­—ç¬¦"""
    if not isinstance(title, str):
        title = str(title)
    cleaned_title = title.replace("\n", " ").replace("\r", " ")
    cleaned_title = re.sub(r"\s+", " ", cleaned_title)
    cleaned_title = cleaned_title.strip()
    return cleaned_title


def ensure_directory_exists(directory: str):
    """ç¡®ä¿ç›®å½•å­˜åœ¨"""
    Path(directory).mkdir(parents=True, exist_ok=True)


def get_output_path(subfolder: str, filename: str) -> str:
    """è·å–è¾“å‡ºè·¯å¾„"""
    date_folder = format_date_folder()
    output_dir = Path("output") / date_folder / subfolder
    ensure_directory_exists(str(output_dir))
    return str(output_dir / filename)


def check_version_update(
    current_version: str, version_url: str, proxy_url: Optional[str] = None
) -> Tuple[bool, Optional[str]]:
    """æ£€æŸ¥ç‰ˆæœ¬æ›´æ–°"""
    try:
        proxies = None
        if proxy_url:
            proxies = {"http": proxy_url, "https": proxy_url}

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept": "text/plain, */*",
            "Cache-Control": "no-cache",
        }

        response = requests.get(
            version_url, proxies=proxies, headers=headers, timeout=10
        )
        response.raise_for_status()

        remote_version = response.text.strip()
        print(f"å½“å‰ç‰ˆæœ¬: {current_version}, è¿œç¨‹ç‰ˆæœ¬: {remote_version}")

        # æ¯”è¾ƒç‰ˆæœ¬
        def parse_version(version_str):
            try:
                parts = version_str.strip().split(".")
                if len(parts) != 3:
                    raise ValueError("ç‰ˆæœ¬å·æ ¼å¼ä¸æ­£ç¡®")
                return int(parts[0]), int(parts[1]), int(parts[2])
            except:
                return 0, 0, 0

        current_tuple = parse_version(current_version)
        remote_tuple = parse_version(remote_version)

        need_update = current_tuple < remote_tuple
        return need_update, remote_version if need_update else None

    except Exception as e:
        print(f"ç‰ˆæœ¬æ£€æŸ¥å¤±è´¥: {e}")
        return False, None


def is_first_crawl_today() -> bool:
    """æ£€æµ‹æ˜¯å¦æ˜¯å½“å¤©ç¬¬ä¸€æ¬¡çˆ¬å–"""
    date_folder = format_date_folder()
    txt_dir = Path("output") / date_folder / "txt"

    if not txt_dir.exists():
        return True

    files = sorted([f for f in txt_dir.iterdir() if f.suffix == ".txt"])
    return len(files) <= 1


def html_escape(text: str) -> str:
    """HTMLè½¬ä¹‰"""
    if not isinstance(text, str):
        text = str(text)

    return (
        text.replace("&", "&amp;")
        .replace("<", "&lt;")
        .replace(">", "&gt;")
        .replace('"', "&quot;")
        .replace("'", "&#x27;")
    )


# ==================== æ–°å¢åŠŸèƒ½å¼€å§‹ ====================
def generate_all_news_html(all_news_results: Dict, id_to_name: Dict) -> str:
    """ç”Ÿæˆå…¨éƒ¨æ–°é—»HTMLé¡µé¢"""
    
    # æŒ‰å¹³å°åˆ†ç»„
    platform_news = {}
    for source_id, titles_data in all_news_results.items():
        source_name = id_to_name.get(source_id, source_id)
        if source_name not in platform_news:
            platform_news[source_name] = []
        
        for title, title_data in titles_data.items():
            ranks = title_data.get("ranks", [])
            url = title_data.get("url", "")
            mobile_url = title_data.get("mobileUrl", "")
            
            platform_news[source_name].append({
                "title": title,
                "ranks": ranks,
                "url": url,
                "mobileUrl": mobile_url
            })
    
    # ç”Ÿæˆ HTML
    html_parts = []
    html_parts.append('''<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>å…¨éƒ¨æ–°é—» - TrendRadar</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { 
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 20px;
        }
        .container {
            max-width: 1400px;
            margin: 0 auto;
            background: white;
            border-radius: 12px;
            padding: 30px;
            box-shadow: 0 10px 40px rgba(0,0,0,0.1);
        }
        h1 { color: #2d3748; margin-bottom: 20px; }
        .stats {
            background: #f7fafc;
            padding: 15px;
            border-radius: 8px;
            margin: 20px 0;
            display: flex;
            gap: 30px;
        }
        .stat-item { display: flex; align-items: center; gap: 10px; }
        .stat-number { font-size: 24px; font-weight: bold; color: #667eea; }
        .platform {
            margin: 30px 0;
            padding: 20px;
            background: #f9fafb;
            border-radius: 8px;
            border-left: 4px solid #667eea;
        }
        .platform h2 { color: #1a202c; margin-bottom: 15px; }
        .news-card {
            background: white;
            border-radius: 8px;
            margin: 12px 0;
            padding: 15px;
            border: 1px solid #e2e8f0;
            cursor: pointer;
            transition: all 0.3s;
        }
        .news-card:hover { box-shadow: 0 4px 12px rgba(102, 126, 234, 0.15); }
        .news-header {
            display: flex;
            align-items: center;
            gap: 12px;
        }
        .news-rank {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 4px 10px;
            border-radius: 6px;
            font-weight: bold;
            min-width: 40px;
            text-align: center;
        }
        .news-rank.top { background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); }
        .news-title { flex: 1; color: #2d3748; font-size: 15px; }
        .news-link { color: #2563eb; text-decoration: none; }
        .news-link:hover { text-decoration: underline; }
        .expand-icon { color: #667eea; transition: transform 0.3s; }
        .news-card.expanded .expand-icon { transform: rotate(180deg); }
        .news-content {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.3s;
            padding-top: 0;
        }
        .news-card.expanded .news-content {
            max-height: 200px;
            padding-top: 10px;
        }
        .filter-bar {
            margin: 20px 0;
            display: flex;
            gap: 10px;
            flex-wrap: wrap;
        }
        .filter-btn {
            padding: 8px 16px;
            background: white;
            border: 2px solid #667eea;
            color: #667eea;
            border-radius: 20px;
            cursor: pointer;
            transition: all 0.3s;
        }
        .filter-btn:hover, .filter-btn.active {
            background: #667eea;
            color: white;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ“° å…¨éƒ¨æ–°é—»</h1>
        <p style="color: #718096; margin-top: 10px;">''')
    
    now = get_beijing_time()
    total_news = sum(len(news_list) for news_list in platform_news.values())
    
    html_parts.append(f'ç”Ÿæˆæ—¶é—´: {now.strftime("%Y-%m-%d %H:%M:%S")}')
    html_parts.append('</p>')
    
    html_parts.append(f'''
        <div class="stats">
            <div class="stat-item">
                <span>ğŸ“Š æ–°é—»æ€»æ•°:</span>
                <span class="stat-number">{total_news}</span>
            </div>
            <div class="stat-item">
                <span>ğŸ¢ å¹³å°æ•°é‡:</span>
                <span class="stat-number">{len(platform_news)}</span>
            </div>
        </div>
        
        <div class="filter-bar">
            <button class="filter-btn active" onclick="showAll()">å…¨éƒ¨å¹³å°</button>''')
    
    # å¹³å°ç­›é€‰æŒ‰é’®
    for platform_name in platform_news.keys():
        safe_name = platform_name.replace("'", "\\'")
        html_parts.append(f'<button class="filter-btn" onclick="filterPlatform(\'{safe_name}\')">{platform_name}</button>')
    
    html_parts.append('</div>')
    
    # ç”Ÿæˆæ¯ä¸ªå¹³å°çš„æ–°é—»
    for platform_name, news_list in sorted(platform_news.items(), key=lambda x: len(x[1]), reverse=True):
        safe_platform = html_escape(platform_name)
        html_parts.append(f'''
        <div class="platform" data-platform="{safe_platform}">
            <h2>ğŸ”¥ {safe_platform} ({len(news_list)} æ¡)</h2>''')
        
        # æ’åº
        sorted_news = sorted(news_list, key=lambda x: x["ranks"][0] if x["ranks"] else 999)
        
        for idx, news_data in enumerate(sorted_news):
            rank = news_data["ranks"][0] if news_data["ranks"] else None
            rank_class = "top" if rank and rank <= 10 else ""
            rank_text = f"#{rank}" if rank else "â€”"
            
            title = html_escape(news_data["title"])
            url = news_data.get("mobileUrl") or news_data.get("url", "")
            
            html_parts.append(f'''
            <div class="news-card" onclick="this.classList.toggle('expanded')">
                <div class="news-header">
                    <span class="news-rank {rank_class}">{rank_text}</span>
                    <div class="news-title">''')
            
            if url:
                safe_url = html_escape(url)
                html_parts.append(f'<a href="{safe_url}" target="_blank" class="news-link" onclick="event.stopPropagation()">{title}</a>')
            else:
                html_parts.append(title)
            
            html_parts.append('''
                    </div>
                    <span class="expand-icon">â–¼</span>
                </div>
                <div class="news-content">
                    <p style="color: #666; font-size: 14px;">ç‚¹å‡»æ ‡é¢˜æŸ¥çœ‹åŸæ–‡</p>
                </div>
            </div>''')
        
        html_parts.append('</div>')
    
    # JavaScript
    html_parts.append('''
    <script>
        function showAll() {
            document.querySelectorAll('.platform').forEach(el => el.style.display = 'block');
            document.querySelectorAll('.filter-btn').forEach(btn => btn.classList.remove('active'));
            event.target.classList.add('active');
        }
        
        function filterPlatform(platform) {
            document.querySelectorAll('.platform').forEach(el => {
                if (el.dataset.platform === platform) {
                    el.style.display = 'block';
                } else {
                    el.style.display = 'none';
                }
            });
            document.querySelectorAll('.filter-btn').forEach(btn => btn.classList.remove('active'));
            event.target.classList.add('active');
        }
    </script>
</body>
</html>''')
    
    # ä¿å­˜æ–‡ä»¶
    date_folder = format_date_folder()
    output_path = Path("output") / date_folder / "html"
    ensure_directory_exists(str(output_path))
    
    file_path = output_path / "all_news.html"
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(html_parts))
    
    print(f"âœ… å…¨éƒ¨æ–°é—»æŠ¥å‘Šå·²ç”Ÿæˆ: {file_path}")
    return str(file_path)
# ==================== æ–°å¢åŠŸèƒ½ç»“æŸ ====================


# === æ¨é€è®°å½•ç®¡ç† ===
class PushRecordManager:
    """æ¨é€è®°å½•ç®¡ç†å™¨"""

    def __init__(self):
        self.record_dir = Path("output") / ".push_records"
        self.ensure_record_dir()
        self.cleanup_old_records()

    def ensure_record_dir(self):
        """ç¡®ä¿è®°å½•ç›®å½•å­˜åœ¨"""
        self.record_dir.mkdir(parents=True, exist_ok=True)

    def get_today_record_file(self) -> Path:
        """è·å–ä»Šå¤©çš„è®°å½•æ–‡ä»¶è·¯å¾„"""
        today = get_beijing_time().strftime("%Y%m%d")
        return self.record_dir / f"push_record_{today}.json"

    def cleanup_old_records(self):
        """æ¸…ç†è¿‡æœŸçš„æ¨é€è®°å½•"""
        retention_days = CONFIG["PUSH_WINDOW"]["RECORD_RETENTION_DAYS"]
        current_time = get_beijing_time()

        for record_file in self.record_dir.glob("push_record_*.json"):
            try:
                date_str = record_file.stem.replace("push_record_", "")
                file_date = datetime.strptime(date_str, "%Y%m%d")
                file_date = pytz.timezone("Asia/Shanghai").localize(file_date)

                if (current_time - file_date).days > retention_days:
                    record_file.unlink()
                    print(f"æ¸…ç†è¿‡æœŸæ¨é€è®°å½•: {record_file.name}")
            except Exception as e:
                print(f"æ¸…ç†è®°å½•æ–‡ä»¶å¤±è´¥ {record_file}: {e}")

    def has_pushed_today(self) -> bool:
        """æ£€æŸ¥ä»Šå¤©æ˜¯å¦å·²ç»æ¨é€è¿‡"""
        record_file = self.get_today_record_file()

        if not record_file.exists():
            return False

        try:
            with open(record_file, "r", encoding="utf-8") as f:
                record = json.load(f)
            return record.get("pushed", False)
        except Exception as e:
            print(f"è¯»å–æ¨é€è®°å½•å¤±è´¥: {e}")
            return False

    def record_push(self, report_type: str):
        """è®°å½•æ¨é€"""
        record_file = self.get_today_record_file()
        now = get_beijing_time()

        record = {
            "pushed": True,
            "push_time": now.strftime("%Y-%m-%d %H:%M:%S"),
            "report_type": report_type,
        }

        try:
            with open(record_file, "w", encoding="utf-8") as f:
                json.dump(record, f, ensure_ascii=False, indent=2)
            print(f"æ¨é€è®°å½•å·²ä¿å­˜: {report_type} at {now.strftime('%H:%M:%S')}")
        except Exception as e:
            print(f"ä¿å­˜æ¨é€è®°å½•å¤±è´¥: {e}")

    def is_in_time_range(self, start_time: str, end_time: str) -> bool:
        """æ£€æŸ¥å½“å‰æ—¶é—´æ˜¯å¦åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å†…"""
        now = get_beijing_time()
        current_time = now.strftime("%H:%M")
    
        def normalize_time(time_str: str) -> str:
            """å°†æ—¶é—´å­—ç¬¦ä¸²æ ‡å‡†åŒ–ä¸º HH:MM æ ¼å¼"""
            try:
                parts = time_str.strip().split(":")
                if len(parts) != 2:
                    raise ValueError(f"æ—¶é—´æ ¼å¼é”™è¯¯: {time_str}")
            
                hour = int(parts[0])
                minute = int(parts[1])
            
                if not (0 <= hour <= 23 and 0 <= minute <= 59):
                    raise ValueError(f"æ—¶é—´èŒƒå›´é”™è¯¯: {time_str}")
            
                return f"{hour:02d}:{minute:02d}"
            except Exception as e:
                print(f"æ—¶é—´æ ¼å¼åŒ–é”™è¯¯ '{time_str}': {e}")
                return time_str
    
        normalized_start = normalize_time(start_time)
        normalized_end = normalize_time(end_time)
        normalized_current = normalize_time(current_time)
    
        result = normalized_start <= normalized_current <= normalized_end
    
        if not result:
            print(f"æ—¶é—´çª—å£åˆ¤æ–­:å½“å‰ {normalized_current},çª—å£ {normalized_start}-{normalized_end}")
    
        return result


# === æ•°æ®è·å– ===
class DataFetcher:
    """æ•°æ®è·å–å™¨"""

    def __init__(self, proxy_url: Optional[str] = None):
        self.proxy_url = proxy_url

    def fetch_data(
        self,
        id_info: Union[str, Tuple[str, str]],
        max_retries: int = 2,
        min_retry_wait: int = 3,
        max_retry_wait: int = 5,
    ) -> Tuple[Optional[str], str, str]:
        """è·å–æŒ‡å®šIDæ•°æ®,æ”¯æŒé‡è¯•"""
        if isinstance(id_info, tuple):
            id_value, alias = id_info
        else:
            id_value = id_info
            alias = id_value

        url = f"https://newsnow.busiyi.world/api/s?id={id_value}&latest"

        proxies = None
        if self.proxy_url:
            proxies = {"http": self.proxy_url, "https": self.proxy_url}

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept": "application/json, text/plain, */*",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Connection": "keep-alive",
            "Cache-Control": "no-cache",
        }

        retries = 0
        while retries <= max_retries:
            try:
                response = requests.get(
                    url, proxies=proxies, headers=headers, timeout=10
                )
                response.raise_for_status()

                data_text = response.text
                data_json = json.loads(data_text)

                status = data_json.get("status", "æœªçŸ¥")
                if status not in ["success", "cache"]:
                    raise ValueError(f"å“åº”çŠ¶æ€å¼‚å¸¸: {status}")

                status_info = "æœ€æ–°æ•°æ®" if status == "success" else "ç¼“å­˜æ•°æ®"
                print(f"è·å– {id_value} æˆåŠŸ({status_info})")
                return data_text, id_value, alias

            except Exception as e:
                retries += 1
                if retries <= max_retries:
                    base_wait = random.uniform(min_retry_wait, max_retry_wait)
                    additional_wait = (retries - 1) * random.uniform(1, 2)
                    wait_time = base_wait + additional_wait
                    print(f"è¯·æ±‚ {id_value} å¤±è´¥: {e}. {wait_time:.2f}ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    print(f"è¯·æ±‚ {id_value} å¤±è´¥: {e}")
                    return None, id_value, alias
        return None, id_value, alias

    def crawl_websites(
        self,
        ids_list: List[Union[str, Tuple[str, str]]],
        request_interval: int = CONFIG["REQUEST_INTERVAL"],
    ) -> Tuple[Dict, Dict, List]:
        """çˆ¬å–å¤šä¸ªç½‘ç«™æ•°æ®"""
        results = {}
        id_to_name = {}
        failed_ids = []

        for i, id_info in enumerate(ids_list):
            if isinstance(id_info, tuple):
                id_value, name = id_info
            else:
                id_value = id_info
                name = id_value

            id_to_name[id_value] = name
            response, _, _ = self.fetch_data(id_info)

            if response:
                try:
                    data = json.loads(response)
                    results[id_value] = {}
                    for index, item in enumerate(data.get("items", []), 1):
                        title = item["title"]
                        url = item.get("url", "")
                        mobile_url = item.get("mobileUrl", "")

                        if title in results[id_value]:
                            results[id_value][title]["ranks"].append(index)
                        else:
                            results[id_value][title] = {
                                "ranks": [index],
                                "url": url,
                                "mobileUrl": mobile_url,
                            }
                except json.JSONDecodeError:
                    print(f"è§£æ {id_value} å“åº”å¤±è´¥")
                    failed_ids.append(id_value)
                except Exception as e:
                    print(f"å¤„ç† {id_value} æ•°æ®å‡ºé”™: {e}")
                    failed_ids.append(id_value)
            else:
                failed_ids.append(id_value)

            if i < len(ids_list) - 1:
                actual_interval = request_interval + random.randint(-10, 20)
                actual_interval = max(50, actual_interval)
                time.sleep(actual_interval / 1000)

        print(f"æˆåŠŸ: {list(results.keys())}, å¤±è´¥: {failed_ids}")
        return results, id_to_name, failed_ids


# === æ•°æ®å¤„ç† ===
def save_titles_to_file(results: Dict, id_to_name: Dict, failed_ids: List) -> str:
    """ä¿å­˜æ ‡é¢˜åˆ°æ–‡ä»¶"""
    file_path = get_output_path("txt", f"{format_time_filename()}.txt")

    with open(file_path, "w", encoding="utf-8") as f:
        for id_value, title_data in results.items():
            # id | name æˆ– id
            name = id_to_name.get(id_value)
            if name and name != id_value:
                f.write(f"{id_value} | {name}\n")
            else:
                f.write(f"{id_value}\n")

            # æŒ‰æ’åæ’åºæ ‡é¢˜
            sorted_titles = []
            for title, info in title_data.items():
                cleaned_title = clean_title(title)
                if isinstance(info, dict):
                    ranks = info.get("ranks", [])
                    url = info.get("url", "")
                    mobile_url = info.get("mobileUrl", "")
                else:
                    ranks = info if isinstance(info, list) else []
                    url = ""
                    mobile_url = ""

                rank = ranks[0] if ranks else 1
                sorted_titles.append((rank, cleaned_title, url, mobile_url))

            sorted_titles.sort(key=lambda x: x[0])

            for rank, cleaned_title, url, mobile_url in sorted_titles:
                line = f"{rank}. {cleaned_title}"

                if url:
                    line += f" [URL:{url}]"
                if mobile_url:
                    line += f" [MOBILE:{mobile_url}]"
                f.write(line + "\n")

            f.write("\n")

        if failed_ids:
            f.write("==== ä»¥ä¸‹IDè¯·æ±‚å¤±è´¥ ====\n")
            for id_value in failed_ids:
                f.write(f"{id_value}\n")

    return file_path


# [åç»­ä»£ç ä¿æŒä¸å˜,ç›´æ¥ç»§ç»­...]
# ç”±äºä»£ç å¤ªé•¿,è¿™é‡Œçœç•¥ä¸­é—´éƒ¨åˆ†,ç›´æ¥è·³åˆ° NewsAnalyzer ç±»çš„ _crawl_data æ–¹æ³•


# === ä¸»åˆ†æå™¨ ===
class NewsAnalyzer:
    """æ–°é—»åˆ†æå™¨"""

    # æ¨¡å¼ç­–ç•¥å®šä¹‰
    MODE_STRATEGIES = {
        "incremental": {
            "mode_name": "å¢é‡æ¨¡å¼",
            "description": "å¢é‡æ¨¡å¼(åªå…³æ³¨æ–°å¢æ–°é—»,æ— æ–°å¢æ—¶ä¸æ¨é€)",
            "realtime_report_type": "å®æ—¶å¢é‡",
            "summary_report_type": "å½“æ—¥æ±‡æ€»",
            "should_send_realtime": True,
            "should_generate_summary": True,
            "summary_mode": "daily",
        },
        "current": {
            "mode_name": "å½“å‰æ¦œå•æ¨¡å¼",
            "description": "å½“å‰æ¦œå•æ¨¡å¼(å½“å‰æ¦œå•åŒ¹é…æ–°é—» + æ–°å¢æ–°é—»åŒºåŸŸ + æŒ‰æ—¶æ¨é€)",
            "realtime_report_type": "å®æ—¶å½“å‰æ¦œå•",
            "summary_report_type": "å½“å‰æ¦œå•æ±‡æ€»",
            "should_send_realtime": True,
            "should_generate_summary": True,
            "summary_mode": "current",
        },
        "daily": {
            "mode_name": "å½“æ—¥æ±‡æ€»æ¨¡å¼",
            "description": "å½“æ—¥æ±‡æ€»æ¨¡å¼(æ‰€æœ‰åŒ¹é…æ–°é—» + æ–°å¢æ–°é—»åŒºåŸŸ + æŒ‰æ—¶æ¨é€)",
            "realtime_report_type": "",
            "summary_report_type": "å½“æ—¥æ±‡æ€»",
            "should_send_realtime": False,
            "should_generate_summary": True,
            "summary_mode": "daily",
        },
    }

    def __init__(self):
        self.request_interval = CONFIG["REQUEST_INTERVAL"]
        self.report_mode = CONFIG["REPORT_MODE"]
        self.rank_threshold = CONFIG["RANK_THRESHOLD"]
        self.is_github_actions = os.environ.get("GITHUB_ACTIONS") == "true"
        self.is_docker_container = self._detect_docker_environment()
        self.update_info = None
        self.proxy_url = None
        self._setup_proxy()
        self.data_fetcher = DataFetcher(self.proxy_url)

        if self.is_github_actions:
            self._check_version_update()

    def _detect_docker_environment(self) -> bool:
        """æ£€æµ‹æ˜¯å¦è¿è¡Œåœ¨ Docker å®¹å™¨ä¸­"""
        try:
            if os.environ.get("DOCKER_CONTAINER") == "true":
                return True

            if os.path.exists("/.dockerenv"):
                return True

            return False
        except Exception:
            return False

    def _should_open_browser(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ‰“å¼€æµè§ˆå™¨"""
        return not self.is_github_actions and not self.is_docker_container

    def _setup_proxy(self) -> None:
        """è®¾ç½®ä»£ç†é…ç½®"""
        if not self.is_github_actions and CONFIG["USE_PROXY"]:
            self.proxy_url = CONFIG["DEFAULT_PROXY"]
            print("æœ¬åœ°ç¯å¢ƒ,ä½¿ç”¨ä»£ç†")
        elif not self.is_github_actions and not CONFIG["USE_PROXY"]:
            print("æœ¬åœ°ç¯å¢ƒ,æœªå¯ç”¨ä»£ç†")
        else:
            print("GitHub Actionsç¯å¢ƒ,ä¸ä½¿ç”¨ä»£ç†")

    def _check_version_update(self) -> None:
        """æ£€æŸ¥ç‰ˆæœ¬æ›´æ–°"""
        try:
            need_update, remote_version = check_version_update(
                VERSION, CONFIG["VERSION_CHECK_URL"], self.proxy_url
            )

            if need_update and remote_version:
                self.update_info = {
                    "current_version": VERSION,
                    "remote_version": remote_version,
                }
                print(f"å‘ç°æ–°ç‰ˆæœ¬: {remote_version} (å½“å‰: {VERSION})")
            else:
                print("ç‰ˆæœ¬æ£€æŸ¥å®Œæˆ,å½“å‰ä¸ºæœ€æ–°ç‰ˆæœ¬")
        except Exception as e:
            print(f"ç‰ˆæœ¬æ£€æŸ¥å‡ºé”™: {e}")

    def _get_mode_strategy(self) -> Dict:
        """è·å–å½“å‰æ¨¡å¼çš„ç­–ç•¥é…ç½®"""
        return self.MODE_STRATEGIES.get(self.report_mode, self.MODE_STRATEGIES["daily"])

    # ... å…¶ä»–æ–¹æ³•ä¿æŒä¸å˜ ...

    def _crawl_data(self) -> Tuple[Dict, Dict, List]:
        """æ‰§è¡Œæ•°æ®çˆ¬å–"""
        ids = []
        for platform in CONFIG["PLATFORMS"]:
            if "name" in platform:
                ids.append((platform["id"], platform["name"]))
            else:
                ids.append(platform["id"])

        print(
            f"é…ç½®çš„ç›‘æ§å¹³å°: {[p.get('name', p['id']) for p in CONFIG['PLATFORMS']]}"
        )
        print(f"å¼€å§‹çˆ¬å–æ•°æ®,è¯·æ±‚é—´éš” {self.request_interval} æ¯«ç§’")
        ensure_directory_exists("output")

        results, id_to_name, failed_ids = self.data_fetcher.crawl_websites(
            ids, self.request_interval
        )

        title_file = save_titles_to_file(results, id_to_name, failed_ids)
        print(f"æ ‡é¢˜å·²ä¿å­˜åˆ°: {title_file}")
        
        # ==================== æ–°å¢:ç”Ÿæˆå…¨éƒ¨æ–°é—»æŠ¥å‘Š ====================
        try:
            generate_all_news_html(results, id_to_name)
        except Exception as e:
            print(f"ç”Ÿæˆå…¨éƒ¨æ–°é—»æŠ¥å‘Šå¤±è´¥: {e}")
        # ==================== æ–°å¢ç»“æŸ ====================

        return results, id_to_name, failed_ids

    # ... å…¶ä½™æ–¹æ³•ä¿æŒå®Œå…¨ä¸å˜,åŒ…æ‹¬ run() ç­‰ ...


def main():
    try:
        analyzer = NewsAnalyzer()
        analyzer.run()
    except FileNotFoundError as e:
        print(f"âŒ é…ç½®æ–‡ä»¶é”™è¯¯: {e}")
        print("\nè¯·ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨:")
        print("  â€¢ config/config.yaml")
        print("  â€¢ config/frequency_words.txt")
        print("\nå‚è€ƒé¡¹ç›®æ–‡æ¡£è¿›è¡Œæ­£ç¡®é…ç½®")
    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œé”™è¯¯: {e}")
        raise


if __name__ == "__main__":
    main()
